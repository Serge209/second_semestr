Норберт Винер - основатель.
Задумали ИИ вместе с фон Нейманом, работал в MIT.
Лето 1956 года Дартмутский Дортмундский семинар по ИИ, заложены основы,
придумано название.
1956-1974 Сделан алгоритм, который доказал все теоремы из учебника геометрии.
Сделали программу, которая обыграла человека в шашки.
Подход экспертных систем, получены прикладные программы для узких областей.
Начало 2000-х ИИ переименовали в “машинное обучение”
В 2010-х годах появился Deep Learning.
В 2015 году был предложен алгоритм, который ошибается меньше чем человек на
картинках.
Гиппокамп - эпизодическая память.Зубчатая фасция - часть гиппокампа, в которой
происходит нейрогенез, рождаются новые нейроны.Ее функция - разделять очень
близкие события, которые произошли. Нейроны передают друг другу сигналы в мозге
при помощи веществ - нейротрансмиттеров.
Нейрон состоит из тела клетки,, дендритного дерева (вход клетки, нос), и аксон,
который позволяет выбросить хим. вещество и подать сигнал другим клеткам.
Меняется потенциал на мембране клетки, чтобы почувствовать этот сигнал,
образуются уплотнения - синапсы. Начинается изменение потенциала на локальной
мембране клетки. Потенциалы суммируются и формируют потенциал действия - и
выбрасывается хим. вещество с другой стороны.
Математическая модель нейрона
На входе - некоторое значение (какую концентрацию хим.вещества выбросили клетки).
Должны учитывать не одно значение концентрации - а много.
Вектор входных значений (коэффициент говорит - насколько хорошо оно передалось
дальше) - входные концентрации.Коэффициент описывает синапс.
Вес связи описывает то, сколько рецепторов есть, насколько чувствителен синапс.
Сигналы со многих клеток (концентрации), перемножаются с локальной
чувствительностью для каждого синапса, все синапсы складываем, нейрон либо
срабатывает либо нет.

W - веса
Ф - нелинейное преобразование
Обучение происходит за счет изменения весов.
Алгоритм обратного распространения ошибки (предложен в 70-е)
![image](https://user-images.githubusercontent.com/80594181/153834049-6a92435f-008a-4bdc-bf4b-6c2326ec1509.png)


2 входные переменные (значения активации) - 1 слой, скрытый слой, выходной слой.
Изначально мы не знаем какие веса, можем задать случайным образом.
Получим значение, которое не является правильным.
Стандартный подход в программировании - есть решение и мы его знаем.

Нейросеть обучается на примерах (не пишется готовый алгоритм, а есть много
примеров).
Пропорционально весам распространяем ошибку.
Производная активационной функции даст знак, с которым распространяется
прибавка.
С 70-х годов вычислительных возможностей не хватало для многих слоев.
Если есть много данных и построим глубокую нейросеть - может быть лучшее
решение. Глубина нейронной сети (большое количество слоев) дала название deep
learning.
3 области применения нейронных сетей: компьютерное зрение, обработка различных
последовательностей (понимание временных зависимостей), управление действиями.
Основной инструмент в компьютерном зрении - сверточные нейронные сети
(функция свертки).

Получается карта признаков.

Значения - это веса. Задача - выучить все паттерны.
Транскрибировать текст - читать по губам.
Можно синтезировать изображение по звуку.
Рекуррентные нейронные сети.

Пример - последовательность слов.
Рекуррентные обладают памятью (информация о том, что сеть видела в предыдущие
моменты - важна сейчас). Нейросеть видит не только текущий кадр, а свое состояние в
предыдущий момент времени.
Юрген Шмитхубер предложил LSTM.
![image](https://user-images.githubusercontent.com/80594181/153835571-df4e7cc6-93a6-4610-ab7f-676ad4a4ce92.png)

Google начал использовать LSTM в переводчике.

Блог Андрея Карпата - аспирант в Стэнфордском университете. Нейросеть можно
натренировать посимвольно.
Тест Тьюринга - говорит машина или человек.
Социальные боты. пример - xiaoice
Нейросетевое обучение с подкреплением.
Применяются, когда нужно чем-то управлять. Есть агент - успех поведения агента
определяется величиной награды.
Стартап Deep Mind (купила Google за 600 млн. долл., стали подразделением Google).
Журнал Nature.
Каспаров проиграл Deep Blue от IBM в 1997 году.
Ветвление позиций в шахматах - в среднем из каждого текущего в 35.
Ли Си Доль - чемпион мира в го.
AlphaGo натренировалась на играх людей, затем играла сама с собой. Затем был
использован метод Монте Карло tree search. Нейросеть научилась выбирать действие
и затем оценивать состояние.
Алгоритм Deep Mind позволил сократить энергопотребление data центров на 40%.
Электричество одна из самых затратных статей расходов Google.
iPavlov создали библиотеку с открытым исходным кодом для создания голосовых
помощников.
Никто не пишет нейросети с нуля, обычно используются уже готовые библиотеки.


![image](https://user-images.githubusercontent.com/80594181/160786801-a37053f0-58cb-4f8b-b2a9-bf715727c638.png)

![image](https://user-images.githubusercontent.com/80594181/160788151-3dda2542-c8d9-4152-99c3-072dc9a414c5.png)



